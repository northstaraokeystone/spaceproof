"""export.py - DOI Archive Generation for Zenodo

THE ZENODO INSIGHT:
    Reproducibility requires immutability.
    A DOI freezes the code and receipts forever.
    Anyone can verify the claims at any time.

Archive Contents:
    - src/ (frozen code)
    - receipts.jsonl (with merkle root)
    - data/synthetic/ (seeded runs, deterministic)
    - data/real/ (hashes only, not full datasets)
    - README.md
    - LICENSE
    - zenodo.json (metadata)

Source: AXIOM Validation Lock v1
"""

import json
import os
import shutil
import tarfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List
import sys

# Add parent directory to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.core import dual_hash, emit_receipt, merkle


# === CONSTANTS ===

TENANT_ID = "axiom-zenodo"
ZENODO_API = "https://zenodo.org/api"

# Files to include in archive
ARCHIVE_INCLUDE = [
    "src/",
    "real_data/",
    "benchmarks/",
    "axiom/",
    "tests/",
    "scripts/",
    "receipts.jsonl",
    "ledger_schema.json",
    "spec.md",
    "CLAUDEME.md",
    "LICENSE",
]

# Files to exclude
ARCHIVE_EXCLUDE = [
    "__pycache__",
    "*.pyc",
    ".git",
    ".github",
    "*.egg-info",
    "real_data/cache/",
]


def freeze_receipts(receipts_path: str = "receipts.jsonl") -> str:
    """Create immutable copy of receipts with merkle root.

    Args:
        receipts_path: Path to receipts file

    Returns:
        Merkle root hash
    """
    receipts = []

    if os.path.exists(receipts_path):
        with open(receipts_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line:
                    try:
                        receipts.append(json.loads(line))
                    except json.JSONDecodeError:
                        pass

    # Compute merkle root
    root = merkle(receipts)

    # Emit freeze receipt
    emit_receipt("receipts_freeze", {
        "tenant_id": TENANT_ID,
        "n_receipts": len(receipts),
        "merkle_root": root,
        "frozen_at": datetime.utcnow().isoformat() + "Z",
    })

    return root


def generate_metadata(
    version: str,
    title: str = "AXIOM Compression System",
    description: str = None,
    creators: List[Dict] = None
) -> Dict:
    """Generate Zenodo metadata JSON.

    Args:
        version: Semantic version string (e.g., "1.1.0")
        title: Archive title
        description: Archive description
        creators: List of {"name": str, "affiliation": str}

    Returns:
        Zenodo metadata dict
    """
    if description is None:
        description = """AXIOM (Autonomous eXecution of Information-theoretic Optimization Methods)

A physics compression framework demonstrating that physical laws are nature's compression algorithms.

Key Features:
- KAN-based symbolic regression for physics discovery
- Real-data validation on SPARC galaxy rotation curves
- Landauer-calibrated bits/kg equivalence for Mars sovereignty
- Receipt-based provenance for every calculation

This archive contains frozen code, receipts, and deterministic synthetic data
for full reproducibility of all claims.
"""

    if creators is None:
        creators = [
            {"name": "AXIOM Contributors", "affiliation": "AXIOM Project"}
        ]

    metadata = {
        "upload_type": "software",
        "publication_date": datetime.utcnow().strftime("%Y-%m-%d"),
        "title": f"{title} v{version}",
        "creators": creators,
        "description": description,
        "access_right": "open",
        "license": "MIT",
        "keywords": [
            "compression",
            "physics",
            "symbolic regression",
            "KAN",
            "Kolmogorov-Arnold Networks",
            "Mars autonomy",
            "information theory",
        ],
        "related_identifiers": [
            {
                "identifier": "https://github.com/axiom-project/axiom",
                "relation": "isSupplementTo",
                "scheme": "url"
            }
        ],
        "version": version,
    }

    return metadata


def create_archive(
    version: str,
    output_path: str = None,
    include_receipts: bool = True,
    include_synthetic: bool = True
) -> str:
    """Bundle code + receipts + seeded runs for Zenodo.

    Args:
        version: Semantic version string
        output_path: Output file path (default: axiom-{version}.tar.gz)
        include_receipts: Whether to include receipts.jsonl
        include_synthetic: Whether to include synthetic data

    Returns:
        Path to created archive

    Archive Contents:
        - src/ (frozen code)
        - receipts.jsonl (with merkle root)
        - data/synthetic/ (seeded runs, deterministic)
        - data/real/ (hashes only, not full datasets)
        - README.md
        - LICENSE
        - zenodo.json (metadata)
    """
    if output_path is None:
        output_path = f"axiom-{version}.tar.gz"

    # Create temp directory for archive staging
    staging_dir = Path(f".zenodo_staging_{version}")
    staging_dir.mkdir(exist_ok=True)

    archive_root = staging_dir / f"axiom-{version}"
    archive_root.mkdir(exist_ok=True)

    # Get project root
    project_root = Path(__file__).parent.parent

    files_included = []

    try:
        # Copy source code
        for include_path in ARCHIVE_INCLUDE:
            src_path = project_root / include_path
            if src_path.exists():
                dst_path = archive_root / include_path

                if src_path.is_dir():
                    shutil.copytree(
                        src_path,
                        dst_path,
                        ignore=shutil.ignore_patterns(*ARCHIVE_EXCLUDE)
                    )
                else:
                    dst_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(src_path, dst_path)

                files_included.append(include_path)

        # Freeze receipts and get merkle root
        merkle_root = ""
        if include_receipts:
            receipts_src = project_root / "receipts.jsonl"
            if receipts_src.exists():
                merkle_root = freeze_receipts(str(receipts_src))
                shutil.copy2(receipts_src, archive_root / "receipts.jsonl")
                files_included.append("receipts.jsonl")

        # Generate synthetic data (deterministic)
        if include_synthetic:
            synthetic_dir = archive_root / "data" / "synthetic"
            synthetic_dir.mkdir(parents=True, exist_ok=True)

            # Generate seeded synthetic galaxies
            try:
                from axiom.cosmos import generate_synthetic_dataset
                import numpy as np

                np.random.seed(42)  # Deterministic seed
                galaxies = generate_synthetic_dataset(n_galaxies=10, seed=42)

                with open(synthetic_dir / "galaxies_seed42.json", 'w') as f:
                    # Convert numpy arrays to lists for JSON
                    galaxies_json = []
                    for g in galaxies:
                        g_json = {
                            "id": g["id"],
                            "regime": g["regime"],
                            "r": g["r"].tolist(),
                            "v": g["v"].tolist(),
                            "v_unc": g["v_unc"].tolist(),
                            "params": g["params"],
                        }
                        galaxies_json.append(g_json)
                    json.dump(galaxies_json, f, indent=2)

                files_included.append("data/synthetic/galaxies_seed42.json")
            except Exception as e:
                print(f"Warning: Could not generate synthetic data: {e}")

        # Create real data hashes (not full datasets)
        real_hashes_dir = archive_root / "data" / "real"
        real_hashes_dir.mkdir(parents=True, exist_ok=True)

        real_data_hashes = {
            "sparc": {
                "description": "SPARC galaxy rotation curves",
                "source": "http://astroweb.cwru.edu/SPARC/",
                "n_galaxies": 175,
                "reference": "Lelli et al. 2016, AJ, 152, 157",
            },
            "moxie": {
                "description": "Perseverance MOXIE telemetry",
                "source": "https://pds-geosciences.wustl.edu/missions/mars2020/",
                "n_runs": 16,
            },
            "eclss": {
                "description": "ISS ECLSS performance data",
                "source": "NASA Technical Reports Server",
                "water_recovery": 0.98,
                "o2_closure": 0.875,
            }
        }

        with open(real_hashes_dir / "data_sources.json", 'w') as f:
            json.dump(real_data_hashes, f, indent=2)
        files_included.append("data/real/data_sources.json")

        # Generate metadata
        metadata = generate_metadata(version)
        with open(archive_root / "zenodo.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        files_included.append("zenodo.json")

        # Compute archive hash
        archive_content = json.dumps({
            "version": version,
            "files": files_included,
            "merkle_root": merkle_root,
        }, sort_keys=True)
        archive_hash = dual_hash(archive_content)

        # Create tarball
        with tarfile.open(output_path, "w:gz") as tar:
            tar.add(archive_root, arcname=f"axiom-{version}")

        # Emit zenodo receipt
        emit_receipt("zenodo", {
            "tenant_id": TENANT_ID,
            "doi": "10.5281/zenodo.XXXXXXX",  # Placeholder until assigned
            "archive_hash": archive_hash,
            "files_included": files_included,
            "version": version,
            "merkle_root": merkle_root,
        })

    finally:
        # Cleanup staging directory
        if staging_dir.exists():
            shutil.rmtree(staging_dir)

    return output_path


def validate_archive(archive_path: str) -> Dict:
    """Validate archive integrity.

    Args:
        archive_path: Path to archive file

    Returns:
        Dict with validation results
    """
    results = {
        "archive_path": archive_path,
        "exists": os.path.exists(archive_path),
        "valid_tarball": False,
        "has_src": False,
        "has_receipts": False,
        "has_zenodo_json": False,
        "files": [],
    }

    if not results["exists"]:
        return results

    try:
        with tarfile.open(archive_path, "r:gz") as tar:
            results["valid_tarball"] = True
            results["files"] = tar.getnames()

            for name in results["files"]:
                if "/src/" in name or name.endswith("/src"):
                    results["has_src"] = True
                if "receipts.jsonl" in name:
                    results["has_receipts"] = True
                if "zenodo.json" in name:
                    results["has_zenodo_json"] = True

    except tarfile.TarError as e:
        results["error"] = str(e)

    return results


# === CLI ENTRY POINT ===

def main():
    """CLI entry point for archive creation."""
    import argparse

    parser = argparse.ArgumentParser(
        description="Create AXIOM Zenodo archive"
    )
    parser.add_argument(
        "version",
        help="Semantic version (e.g., 1.1.0)"
    )
    parser.add_argument(
        "--output", "-o",
        help="Output file path"
    )
    parser.add_argument(
        "--no-receipts",
        action="store_true",
        help="Exclude receipts.jsonl"
    )
    parser.add_argument(
        "--no-synthetic",
        action="store_true",
        help="Exclude synthetic data"
    )
    parser.add_argument(
        "--validate", "-v",
        action="store_true",
        help="Validate archive after creation"
    )

    args = parser.parse_args()

    print(f"Creating AXIOM archive v{args.version}...")

    archive_path = create_archive(
        version=args.version,
        output_path=args.output,
        include_receipts=not args.no_receipts,
        include_synthetic=not args.no_synthetic,
    )

    print(f"Archive created: {archive_path}")

    if args.validate:
        results = validate_archive(archive_path)
        print("\nValidation:")
        print(f"  Valid tarball: {results['valid_tarball']}")
        print(f"  Has src/: {results['has_src']}")
        print(f"  Has receipts: {results['has_receipts']}")
        print(f"  Has zenodo.json: {results['has_zenodo_json']}")
        print(f"  Total files: {len(results['files'])}")


if __name__ == "__main__":
    main()
